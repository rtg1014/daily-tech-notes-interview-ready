

### 대규모 트래픽 환경에서 캐시를 운용하는데, Cache Aside(캐시 미스 발생 시 적재) 전략을 사용한다고 가정하겠습니다. 
- 이때, 수많은 요청들이 동시에 캐시 미스를 확인하고 원본 저장소에서 데이터를 가져와 캐시에 적재하는 상황이 발생할 수 있는데요. 이를 캐시 스탬피드 현상 혹은 Thundering Herd 문제라고 표현합니다.
- 캐시 스탬피드 현상은 원본 데이터베이스와 캐시의 성능을 저하할 수도 있습니다.

  ![d914948a-3d57-429c-9720-108d2c989cf5](https://github.com/user-attachments/assets/00b910a4-4934-4f7d-b841-68679467a45c)

<br>

---

<br>



## 이 문제는 어떻게 풀어볼 수 있을까요? 
- 해당 방식은 크게 잠금, 외부 재계산, 확률적 조기 재계산 방식으로 풀어볼 수 있습니다.



---

### 잠금(Locking) 방식
- 잠금(Locking) 방식 은 한 요청 처리 스레드가 해당 캐시 키에 대한 잠금을 획득합니다. 
- 이로인해 다른 요청 처리 스레드들은 잠시 대기합니다.
- 잠금을 획득한 스레드는 사용자 요청에 응답하는 과정동안 캐시 적재 작업은 비동기 스레드로 처리할 수 있습니다.
- 잠금을 사용하기 때문에 성능 저하 가능성이 존재하며, 잠금 획득 스레드의 실패, 잠금의 생명 주기, 데드락 등 다양한 상황을 고려해야한다는 단점이 존재합니다.


---


### 외부 재계산(External Recomputation) 방식
- 외부 재계산(External Recomputation) 방식 은 모든 요청 처리 스레드가 캐시 적재를 수행하지 않습니다. 
- 대신, 캐시를 주기적으로 모니터링하는 스레드를 별도로 관리하여 캐시의 만료시간이 얼마 남지 않은 경우, 데이터를 갱신하여 문제를 예방합니다.
- 해당 방식은 다시 사용되지 않을 데이터를 포함하여 갱신하기 때문에 메모리에 대한 불필요한 연산이 발생하고, 메모리 공간을 비효율적으로 사용할 가능성이 존재합니다.

---

### 확률적 조기 재계산(Probablistic Early Recomputation) 방식 
- 캐시 만료 시간이 얼마 남지 않았을 경우, 확률이라는 개념을 사용하여 여러 요청 처리 스레드 중에서 적은 수만이 캐시를 적재하는 작업을 수행하여 스탬피드 현상을 완화할 수 있습니다.



---

<Br>

> ## 추가 학습자료 (GPT)

---

<Br><Br>



### 🔑 **캐시 스탬피드(Cache Stampede)란?**

---

캐시 스탬피드(Cache Stampede)는 다수의 클라이언트가 동시에 **만료된 캐시 데이터를 요청**하여, 데이터베이스(DB)나 백엔드 시스템에 과도한 부하를 발생시키는 현상입니다.

---

### 📚 **현상 설명**
1. **캐시 만료**: 특정 데이터가 캐시에서 만료됨.
2. **동시 요청 발생**: 여러 클라이언트가 동일한 캐시 키를 요청.
3. **백엔드 부하 증가**: 캐시가 만료된 상태이므로 요청이 모두 DB나 백엔드로 전달.
4. **연쇄적인 성능 저하**: DB 또는 서버의 성능이 급격히 저하되고, 서비스 장애로 이어질 수 있음.

---

### 🖼 **실생활 비유**
- **비유**: 새벽에 도넛 가게가 문을 열었는데, 갑자기 많은 사람들이 몰려와 도넛을 주문하는 상황.  
  - 도넛(캐시)이 다 떨어진 상태에서 한 명 한 명에게 새로 도넛을 만들어줘야 함(DB 부하).  
  - 도넛 가게 주인이 과로로 쓰러질 위험(DB 과부하).  

---

### 🛠 **캐시 스탬피드가 발생하는 이유**
1. **캐시 만료 시점 동기화**: 동일한 데이터의 캐시 만료 시점이 여러 클라이언트에 대해 동기화됨.
2. **높은 동시성 요청**: 인기 있는 키(핫 키)에 대한 동시 요청이 많음.
3. **캐시 적중률 감소**: 캐시 만료 후 데이터가 새로 저장되기 전까지 모든 요청이 백엔드로 전달됨.

---

## **2️⃣ 캐시 스탬피드 해결 방법**

캐시 스탬피드를 방지하기 위한 몇 가지 전략과 기법을 소개합니다.

---

### **1. 캐시 갱신을 분산시키기 (Cache Expiry Jitter)**
- **개념**: 캐시 만료 시간을 고정하지 않고, 랜덤한 값을 추가하여 **만료 시점을 분산**.
- **방법**:
  - 예: 기본 만료 시간 + 0~10초 사이의 랜덤 값.
- **효과**: 모든 캐시가 동시에 만료되지 않아 동시 요청을 방지.

#### **예제 코드 (Java)**
```java
import java.util.Random;

int baseExpiry = 3600; // 1시간
int jitter = new Random().nextInt(600); // 최대 10분 추가
int finalExpiry = baseExpiry + jitter;
```

---

### **2. 캐시 미리 로드 (Cache Preloading)**
- **개념**: 캐시가 만료되기 전에 **주기적으로 갱신**하여 만료 상태를 방지.
- **방법**:
  - 백그라운드 작업(스케줄러)을 사용하여 캐시를 미리 갱신.
- **효과**: 클라이언트 요청이 백엔드로 전달되기 전에 새로운 데이터를 캐시에 저장.

#### **예제 코드 (Spring Scheduler)**
```java
import org.springframework.scheduling.annotation.Scheduled;
import org.springframework.stereotype.Service;

@Service
public class CachePreloadingService {

    @Scheduled(fixedRate = 3600000) // 1시간마다 실행
    public void preloadCache() {
        String data = fetchDataFromDB(); // DB에서 데이터 가져오기
        cache.put("myKey", data);       // 캐시에 저장
    }

    private String fetchDataFromDB() {
        // 데이터베이스 조회 로직
        return "Some data from DB";
    }
}
```

---

### **3. Mutex 잠금 (Cache Locking)**
- **개념**: 캐시가 만료된 상태에서 **한 클라이언트만 데이터 갱신**을 수행하도록 잠금을 설정.
- **방법**:
  - 다른 요청은 잠금을 기다리거나 기존 캐시 데이터를 사용.
- **효과**: 한 번에 하나의 요청만 DB에 접근하여 부하를 방지.

#### **예제 코드 (Redis)**

```java
import org.springframework.data.redis.core.RedisTemplate;
import org.springframework.stereotype.Service;

@Service
public class CacheWithMutex {

    private final RedisTemplate<String, String> redisTemplate;

    public CacheWithMutex(RedisTemplate<String, String> redisTemplate) {
        this.redisTemplate = redisTemplate;
    }

    public String getData(String key) {
        String value = redisTemplate.opsForValue().get(key);

        if (value == null) {
            // 캐시가 없으면 잠금 시도
            boolean lockAcquired = redisTemplate.opsForValue().setIfAbsent("lock:" + key, "1", 5, TimeUnit.SECONDS);
            if (lockAcquired) {
                try {
                    // 데이터 갱신
                    value = fetchDataFromDB();
                    redisTemplate.opsForValue().set(key, value, 1, TimeUnit.HOURS);
                } finally {
                    // 잠금 해제
                    redisTemplate.delete("lock:" + key);
                }
            } else {
                // 잠금 대기
                while ((value = redisTemplate.opsForValue().get(key)) == null) {
                    try {
                        Thread.sleep(50); // 짧은 대기
                    } catch (InterruptedException e) {
                        Thread.currentThread().interrupt();
                    }
                }
            }
        }

        return value;
    }

    private String fetchDataFromDB() {
        // 데이터베이스 조회 로직
        return "Some data from DB";
    }
}
```

---

### **4. 이중 캐시(Double Caching)**
- **개념**: 캐시 데이터를 **1차 캐시(임시)**와 **2차 캐시(주 캐시)**로 나누어 관리.
- **방법**:
  - 1차 캐시: 오래된 데이터를 유지하며 즉시 반환.
  - 2차 캐시: 새 데이터를 갱신 중.
- **효과**: 데이터 갱신 중에도 클라이언트는 1차 캐시 데이터를 사용할 수 있음.

---

### **5. 요청 결과 캐싱**
- **개념**: 백엔드가 동일한 작업을 반복하지 않도록, 요청의 결과를 캐시.
- **방법**:
  - 동일 요청은 기존 응답을 반환.
  - 예: **Promise** 캐싱(Promise를 재활용).

#### **Node.js 예제**
```javascript
const cache = {};
let promise = null;

async function fetchDataWithCache(key) {
    if (cache[key]) {
        return cache[key];
    }

    if (!promise) {
        promise = fetchFromDB().then((data) => {
            cache[key] = data;
            promise = null;
            return data;
        });
    }

    return promise;
}

async function fetchFromDB() {
    // DB 호출
    return "Data from DB";
}
```

---

## **3️⃣ 캐시 스탬피드 방지가 중요한 서비스**

### **1. 트래픽이 많은 서비스**
- 소셜 네트워크 서비스(SNS): 인기 게시글 조회.
- 이커머스: 인기 상품 정보 조회.

### **2. 실시간 데이터가 필요한 서비스**
- 금융 데이터: 주식, 환율.
- 스포츠 경기: 실시간 점수.

### **3. 데이터 갱신 주기가 긴 서비스**
- 공통 데이터: 국가 코드, 카테고리 목록.

---

## **4️⃣ 결론**
- **캐시 스탬피드**는 다수의 동시 요청이 캐시 만료 시점을 기준으로 백엔드 부하를 유발하는 문제입니다.
- 해결책:
  1. **캐시 갱신 분산** (Expiry Jitter).
  2. **캐시 미리 로드** (Preloading).
  3. **Mutex 잠금** (Cache Locking).
  4. **이중 캐시** (Double Caching).
- 서비스의 트래픽 특성과 데이터 특성을 기반으로 적합한 전략을 선택해야 합니다.



